# 02) Data Ingestion

This component handles the critical first step of capturing the high-volume, real-time data stream generated by the user simulation. Google Cloud Pub/Sub was chosen as the ingestion service for its massive scalability, reliability, and seamless integration with the GCP ecosystem.

> Google Cloud Pub/Sub's autoscaling allows for seamless ingestion and delivery of messages from all upstream and to all downstream services!!! ðŸŽ‰ðŸŽ‰


The ingestion layer consists of two primary components:

- **ðŸ“¥ Publishers:** The containerized Python applications running on GKE publish messages to this topic in efficient batches.

- **ðŸ“¤ Subscribers:** Downstream services (like the Dataflow pipeline) pull data from subscriptions to this topic, allowing for decoupled and parallel processing.


## Step 0: Why Pub/Sub?

While alternatives like _Apache Kafka_ are powerful, I chose **Google Cloud Pub/Sub** for this project because it:

- **Decouples Systems:** It acts as a message bus that completely separates the data-producing applications (running on GKE) from the data-consuming applications (Dataflow).

- **Handles Backpressure:** If a downstream service like Dataflow slows down, Pub/Sub automatically retains messages, preventing data loss.

- **Is Fully Managed & Serverless:** It scales automatically from zero to millions of messages per second with no operational overhead, allowing the focus to remain on application logic.



## Step 1: Just make it!

You can setup the Pub/Sub and create a subscription from the Google Cloud Console.

> Just make sure to setup the instance in the same region as other services, e.g. `us-east1`


## Step 2: Schema Enforcement with Avro

To ensure data quality and create a reliable "data contract" between services, an **Avro schema** was attached to the Pub/Sub topic. This enforces that all published messages conform to the expected structure, preventing corrupted or malformed data from entering the pipeline.

The Avro schema for the project can be found at (same folder): `02-data-ingestion/user_interaction_v1_avro.json`. I've only included one of each data types defined in the schema.
```json
{
    "type": "record",
    "name": "UserInteractionSchema",
    "namespace": "com.appstore.events",
    "doc": "Defines the structure for a user interaction event in the AppStore simulation.",
    "fields": [
        {
            "name": "generation_timestamp",
            "type": {
              "type": "long",
              "logicalType": "timestamp-micros"
            },
            "doc": "Unix timestamp (microseconds since epoch) when the event was generated by the publisher."
        },
        {
            "name": "event_id",
            "type": "string",
            "doc": "Unique identifier for the event (UUID format)."
        },
        // ...
        {
            "name": "event_type",
            "type": {
                "type": "enum",
                "name": "EventType",
                "symbols": [
                    "app_open",
                    "app_close",
                    "search",
                    "app_install",
                    "app_uninstall",
                    "review_submit",
                    "in_app_purchase"
                ]
            },
            "doc": "The type of interaction."
        },
        // ...
        {
            "name": "event_details",
            "type": "string",
            "doc": "A JSON string representing event-specific data."
        }
    ]
}
```
