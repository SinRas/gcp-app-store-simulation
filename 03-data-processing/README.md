# 03) Data processing

I've used two ways of processing the Data:

1. Use the Pub/Sub's subscription to automatically "Deliver to BigQuery table". This is most convenient if the Avro schema of your topic translates well to the BigQuery Table schema you are using an no exta processing is needed.

2. Pull the data from Pub/Sub and process it with Apache Beam and store it as Parquet files on GCS. The runner is managed by GCP's Dataflow.


## Option 1: Using GCP's internal "Delivery" mechanism

When you already have created an Avro schema for your Pub/Sub topic, if that's already compatible the BigQuery table's schema and no further processing is needed, then you can setup Pub/Sub subscription to do the processing for you.

You just need to make sure the Avro schema is compatible with BigQuery's table schema. For example you can checkout the files `02-data-ingestion/user_interaction_v1_avro.json` and `03-data-warehouse/00_create_main_table.sql` and see their compatibilities.  
A few things to highlight are:

```json
// Avro
{
    "name": "generation_timestamp",
    "type": {
        "type": "long",
        "logicalType": "timestamp-micros"
    },
    "doc": "Unix timestamp (microseconds since epoch) when the event was generated by the publisher."
}
// BigQuery sql
generation_timestamp TIMESTAMP OPTIONS(
    description = "Timestamp when the event was generated by the publisher."
)
```

```json
// Avro
{
    "name": "event_type",
    "type": {
        "type": "enum",
        "name": "EventType",
        "symbols": [
            "app_open",
            "app_close",
            "search",
            "app_install",
            "app_uninstall",
            "review_submit",
            "in_app_purchase"
        ]
    },
    "doc": "The type of interaction."
}
// BigQuery sql
event_type STRING
```
